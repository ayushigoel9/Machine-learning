{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re \n",
    "import numpy as np\n",
    "np.random.seed(0) # ensure reproducibility\n",
    "np.set_printoptions(suppress = True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import log_loss\n",
    "# Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# NN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Stacking\n",
    "from vecstack import stacking\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target'] \n",
    "X = train.drop([\"ID_code\", \"target\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (160000, 200)\n",
      "Test shape:  (40000, 200)\n"
     ]
    }
   ],
   "source": [
    "n_classes = 3\n",
    "\n",
    "# Create data: 500 example, 5 feature, 3 classes\n",
    "X, y = make_classification(n_samples=200000, n_features=200, \n",
    "                           n_informative=3, n_redundant=1, \n",
    "                           n_classes=n_classes, flip_y=0, \n",
    "                           random_state=0)\n",
    "\n",
    "# Make train/test split\n",
    "# As usual in machine learning task we have X_train, y_train, and X_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, \n",
    "                    input_dim=X_train.shape[1], \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(n_classes, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Caution! All models and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended.\n",
    "models_1 = [ \n",
    "    GaussianNB(),\n",
    "    \n",
    "    LogisticRegression(random_state=0),\n",
    "    \n",
    "    ExtraTreesClassifier(random_state=0, n_jobs=-1, \n",
    "                         n_estimators=100, max_depth=3),\n",
    "                         \n",
    "    RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                           n_estimators=100, max_depth=3),\n",
    "        \n",
    "    XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                  n_estimators=100, max_depth=3),\n",
    "                  \n",
    "    LGBMClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                   n_estimators=100, max_depth=3),\n",
    "                  \n",
    "    KerasClassifier(build_fn=build_keras_model_1, epochs=2, \n",
    "                    batch_size=32, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred]\n",
      "n_models:     [7]\n",
      "\n",
      "model  0:     [GaussianNB]\n",
      "    fold  0:  [0.60107452]\n",
      "    fold  1:  [0.62014706]\n",
      "    fold  2:  [0.61672544]\n",
      "    fold  3:  [0.61218361]\n",
      "    fold  4:  [0.61952161]\n",
      "    ----\n",
      "    MEAN:     [0.61393045] + [0.00701492]\n",
      "    FULL:     [0.61393026]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  1:     [LogisticRegression]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  0:  [0.56465399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  1:  [0.58025074]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  2:  [0.57469431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  3:  [0.57474876]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold  4:  [0.58078008]\n",
      "    ----\n",
      "    MEAN:     [0.57502557] + [0.00579953]\n",
      "    FULL:     [0.57502541]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  2:     [ExtraTreesClassifier]\n",
      "    fold  0:  [1.06074423]\n",
      "    fold  1:  [1.06235617]\n",
      "    fold  2:  [1.04947944]\n",
      "    fold  3:  [1.06130249]\n",
      "    fold  4:  [1.06374782]\n",
      "    ----\n",
      "    MEAN:     [1.05952603] + [0.00512654]\n",
      "    FULL:     [1.05952601]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  3:     [RandomForestClassifier]\n",
      "    fold  0:  [0.92345645]\n",
      "    fold  1:  [0.90625637]\n",
      "    fold  2:  [0.92339716]\n",
      "    fold  3:  [0.91650931]\n",
      "    fold  4:  [0.91757033]\n",
      "    ----\n",
      "    MEAN:     [0.91743792] + [0.00628716]\n",
      "    FULL:     [0.91743800]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  4:     [XGBClassifier]\n",
      "    fold  0:  [0.43016298]\n",
      "    fold  1:  [0.43888331]\n",
      "    fold  2:  [0.43915033]\n",
      "    fold  3:  [0.43740928]\n",
      "    fold  4:  [0.44352498]\n",
      "    ----\n",
      "    MEAN:     [0.43782618] + [0.00434156]\n",
      "    FULL:     [0.43782605]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  5:     [LGBMClassifier]\n",
      "    fold  0:  [0.43093772]\n",
      "    fold  1:  [0.43751131]\n",
      "    fold  2:  [0.43960360]\n",
      "    fold  3:  [0.43950096]\n",
      "    fold  4:  [0.44295717]\n",
      "    ----\n",
      "    MEAN:     [0.43810215] + [0.00398649]\n",
      "    FULL:     [0.43810202]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  6:     [KerasClassifier]\n",
      "WARNING:tensorflow:From C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Ayushi.Goel\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "    fold  0:  [0.45651245]\n",
      "    fold  1:  [0.46495099]\n",
      "    fold  2:  [0.46267909]\n",
      "    fold  3:  [0.46116035]\n",
      "    fold  4:  [0.47025072]\n",
      "    ----\n",
      "    MEAN:     [0.46311072] + [0.00451428]\n",
      "    FULL:     [0.46311060]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "Result was saved to [.\\[2019.04.10].[17.49.09].227856.a81bd6.npy]\n"
     ]
    }
   ],
   "source": [
    "S_train_1, S_test_1 = stacking(models_1,                   # list of models\n",
    "                               X_train, y_train, X_test,   # data\n",
    "                               regression=False,           # classification task (if you need \n",
    "                                                           #     regression - set to True)\n",
    "                               mode='oof_pred',            # mode: oof for train set, fit on full \n",
    "                                                           #     train and predict test set once\n",
    "                               needs_proba=True,           # predict probabilities (if you need \n",
    "                                                           #     class labels - set to False) \n",
    "                               save_dir='.',               # save result and log in current dir \n",
    "                                                           #     (to disable saving - set to None)\n",
    "                               metric=log_loss,            # metric: callable\n",
    "                               n_folds=5,                  # number of folds\n",
    "                               stratified=True,            # stratified split for folds\n",
    "                               shuffle=True,               # shuffle the data\n",
    "                               random_state=0,             # ensure reproducibility\n",
    "                               verbose=2)                  # print all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 classes and 7 models so in resulting arrays we expect to see 21 columns.\n",
      "S_train_1 shape: (160000, 21)\n",
      "S_test_1 shape:  (40000, 21)\n"
     ]
    }
   ],
   "source": [
    "print('We have %d classes and %d models so in resulting arrays \\\n",
    "we expect to see %d columns.' % (n_classes, len(models_1), n_classes * len(models_1)))\n",
    "print('S_train_1 shape:', S_train_1.shape)\n",
    "print('S_test_1 shape: ', S_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98996697, 0.0014692 , 0.00856383, 0.90755247, 0.06482565,\n",
       "        0.02762189, 0.3509121 , 0.32470061, 0.32438728, 0.47477919,\n",
       "        0.26467815, 0.26054266, 0.98931456, 0.00764197, 0.00304349,\n",
       "        0.98757663, 0.00978849, 0.00263487, 0.99689096, 0.0031091 ,\n",
       "        0.00000001],\n",
       "       [0.11227576, 0.65222713, 0.23549711, 0.17150716, 0.50036272,\n",
       "        0.32813011, 0.33411975, 0.33864272, 0.32723754, 0.28745166,\n",
       "        0.41626246, 0.29628587, 0.15967681, 0.57350618, 0.266817  ,\n",
       "        0.15763446, 0.59017378, 0.25219176, 0.32695445, 0.43366429,\n",
       "        0.23938128]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19756544, 0.69596677, 0.1064678 , 0.2823499 , 0.57946909,\n",
       "        0.138181  , 0.33480539, 0.34222773, 0.32296688, 0.29637844,\n",
       "        0.42388314, 0.27973842, 0.22797398, 0.64218599, 0.12984006,\n",
       "        0.24673295, 0.62236016, 0.13090689, 0.13497773, 0.68432266,\n",
       "        0.18069959],\n",
       "       [0.10157364, 0.61288917, 0.28553719, 0.14327163, 0.49265634,\n",
       "        0.36407202, 0.32829869, 0.34242709, 0.32927422, 0.29610252,\n",
       "        0.41331745, 0.29058003, 0.14889233, 0.58053714, 0.27057058,\n",
       "        0.14877604, 0.58005014, 0.27117382, 0.15368925, 0.41763222,\n",
       "        0.42867848]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays:\n",
      "[2019.04.10].[17.49.09].227856.a81bd6.npy\n",
      "[2019.04.10].[17.53.42].608953.073cde.npy\n",
      "\n",
      "Logs:\n",
      "[2019.04.10].[17.49.09].227856.a81bd6.log.txt\n",
      "[2019.04.10].[17.53.42].608953.073cde.log.txt\n"
     ]
    }
   ],
   "source": [
    "names = sorted(glob('*.npy'))\n",
    "npy_1_name = names[0] # for later use\n",
    "\n",
    "print('Arrays:')\n",
    "for name in names:\n",
    "    print(name)\n",
    "\n",
    "names = sorted(glob('*.log.txt'))\n",
    "log_1_name = names[0] # for later use\n",
    "\n",
    "print('\\nLogs:')\n",
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, \n",
    "                    input_dim=X_train.shape[1], \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(64, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(n_classes, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Caution! All models and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended.\n",
    "models_2 = [        \n",
    "    KerasClassifier(build_fn=build_keras_model_2, epochs=5, \n",
    "                    batch_size=32, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred]\n",
      "n_models:     [1]\n",
      "\n",
      "model  0:     [KerasClassifier]\n",
      "    fold  0:  [0.45979184]\n",
      "    fold  1:  [0.49483895]\n",
      "    fold  2:  [0.46820496]\n",
      "    fold  3:  [0.48360031]\n",
      "    fold  4:  [0.49763215]\n",
      "    ----\n",
      "    MEAN:     [0.48081364] + [0.01475284]\n",
      "    FULL:     [0.48081326]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "Result was saved to [.\\[2019.04.10].[17.53.42].608953.073cde.npy]\n"
     ]
    }
   ],
   "source": [
    "S_train_2, S_test_2 = stacking(models_2,                   # list of models\n",
    "                               X_train, y_train, X_test,   # data\n",
    "                               regression=False,           # classification task (if you need \n",
    "                                                           #     regression - set to True)\n",
    "                               mode='oof_pred',            # mode: oof for train set, fit on full \n",
    "                                                           #     train and predict test set once\n",
    "                               needs_proba=True,           # predict probabilities (if you need \n",
    "                                                           #     class labels - set to False) \n",
    "                               save_dir='.',               # save result and log in current dir \n",
    "                                                           #     (to disable saving - set to None)\n",
    "                               metric=log_loss,            # metric: callable\n",
    "                               n_folds=5,                  # number of folds\n",
    "                               stratified=True,            # stratified split for folds\n",
    "                               shuffle=True,               # shuffle the data\n",
    "                               random_state=0,             # ensure reproducibility\n",
    "                               verbose=2)                  # print all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening this log: [2019.04.10].[17.49.09].227856.a81bd6.log.txt\n",
      "models build in those session.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"opening this log: %s\" % log_1_name)\n",
    "with open(log_1_name) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"models build in those session.\\n\")\n",
    "for line in lines:\n",
    "    if re.search(r'^model [0-9]+', line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .npy file: [2019.04.10].[17.49.09].227856.a81bd6.npy\n"
     ]
    }
   ],
   "source": [
    "print(\" .npy file: %s\" % npy_1_name)\n",
    "S = np.load(npy_1_name)\n",
    "S_train_lgbm = S[0][:, 15:18]\n",
    "S_test_lgbm = S[1][:, 15:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98757663, 0.00978849, 0.00263487],\n",
       "       [0.15763446, 0.59017378, 0.25219176],\n",
       "       [0.28626021, 0.27927242, 0.43446736],\n",
       "       [0.63731884, 0.30184423, 0.06083693],\n",
       "       [0.86489797, 0.03197194, 0.10313009]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train_lgbm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24673295, 0.62236016, 0.13090689],\n",
       "       [0.14877604, 0.58005014, 0.27117382],\n",
       "       [0.0518705 , 0.01172231, 0.93640719],\n",
       "       [0.00227678, 0.02780158, 0.96992164],\n",
       "       [0.04190374, 0.49007427, 0.46802199]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test_lgbm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMCLassifier log loss: 0.43810202\n"
     ]
    }
   ],
   "source": [
    "print('LGBMCLassifier log loss: %.8f' % log_loss(y_train, S_train_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 classes and 8 models TOTAL so in resulting arrays we expect to see 24 columns.\n"
     ]
    }
   ],
   "source": [
    "print('We have %d classes and %d models TOTAL so in resulting arrays \\\n",
    "we expect to see %d columns.' % (n_classes, len(models_1) + len(models_2), \n",
    "                                 n_classes * (len(models_1) + len(models_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: [2019.04.10].[17.49.09].227856.a81bd6.npy\n",
      "Loading: [2019.04.10].[17.53.42].608953.073cde.npy\n",
      "\n",
      "S_train_all shape: (160000, 24)\n",
      "S_test_all shape:  (40000, 24)\n"
     ]
    }
   ],
   "source": [
    "# Create empty arrays\n",
    "S_train_all = np.zeros((X_train.shape[0], 0))\n",
    "S_test_all = np.zeros((X_test.shape[0], 0))\n",
    "\n",
    "# Load results\n",
    "for name in sorted(glob('*.npy')):\n",
    "    print('Loading: %s' % name)\n",
    "    S = np.load(name)\n",
    "    S_train_all = np.c_[S_train_all, S[0]]\n",
    "    S_test_all = np.c_[S_test_all, S[1]]\n",
    "    \n",
    "print('\\nS_train_all shape:', S_train_all.shape)\n",
    "print('S_test_all shape: ', S_test_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction score: 0.39562066\n"
     ]
    }
   ],
   "source": [
    "# Initialize 2nd level model\n",
    "model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                      n_estimators=100, max_depth=3)\n",
    "    \n",
    "# Fit 2nd level model\n",
    "model = model.fit(S_train_all, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict_proba(S_test_all)\n",
    "\n",
    "# Final prediction score\n",
    "print('Final prediction score: %.8f' % log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23'] ['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9', 'var_10', 'var_11', 'var_12', 'var_13', 'var_14', 'var_15', 'var_16', 'var_17', 'var_18', 'var_19', 'var_20', 'var_21', 'var_22', 'var_23', 'var_24', 'var_25', 'var_26', 'var_27', 'var_28', 'var_29', 'var_30', 'var_31', 'var_32', 'var_33', 'var_34', 'var_35', 'var_36', 'var_37', 'var_38', 'var_39', 'var_40', 'var_41', 'var_42', 'var_43', 'var_44', 'var_45', 'var_46', 'var_47', 'var_48', 'var_49', 'var_50', 'var_51', 'var_52', 'var_53', 'var_54', 'var_55', 'var_56', 'var_57', 'var_58', 'var_59', 'var_60', 'var_61', 'var_62', 'var_63', 'var_64', 'var_65', 'var_66', 'var_67', 'var_68', 'var_69', 'var_70', 'var_71', 'var_72', 'var_73', 'var_74', 'var_75', 'var_76', 'var_77', 'var_78', 'var_79', 'var_80', 'var_81', 'var_82', 'var_83', 'var_84', 'var_85', 'var_86', 'var_87', 'var_88', 'var_89', 'var_90', 'var_91', 'var_92', 'var_93', 'var_94', 'var_95', 'var_96', 'var_97', 'var_98', 'var_99', 'var_100', 'var_101', 'var_102', 'var_103', 'var_104', 'var_105', 'var_106', 'var_107', 'var_108', 'var_109', 'var_110', 'var_111', 'var_112', 'var_113', 'var_114', 'var_115', 'var_116', 'var_117', 'var_118', 'var_119', 'var_120', 'var_121', 'var_122', 'var_123', 'var_124', 'var_125', 'var_126', 'var_127', 'var_128', 'var_129', 'var_130', 'var_131', 'var_132', 'var_133', 'var_134', 'var_135', 'var_136', 'var_137', 'var_138', 'var_139', 'var_140', 'var_141', 'var_142', 'var_143', 'var_144', 'var_145', 'var_146', 'var_147', 'var_148', 'var_149', 'var_150', 'var_151', 'var_152', 'var_153', 'var_154', 'var_155', 'var_156', 'var_157', 'var_158', 'var_159', 'var_160', 'var_161', 'var_162', 'var_163', 'var_164', 'var_165', 'var_166', 'var_167', 'var_168', 'var_169', 'var_170', 'var_171', 'var_172', 'var_173', 'var_174', 'var_175', 'var_176', 'var_177', 'var_178', 'var_179', 'var_180', 'var_181', 'var_182', 'var_183', 'var_184', 'var_185', 'var_186', 'var_187', 'var_188', 'var_189', 'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195', 'var_196', 'var_197', 'var_198', 'var_199']\nexpected f16, f11, f5, f10, f15, f14, f23, f18, f2, f1, f4, f12, f21, f22, f20, f9, f8, f0, f3, f19, f13, f17, f7, f6 in input data\ntraining data did not have the following fields: var_78, var_114, var_9, var_19, var_147, var_177, var_134, var_90, var_21, var_30, var_141, var_67, var_102, var_151, var_133, var_136, var_188, var_195, var_113, var_149, var_43, var_2, var_126, var_112, var_10, var_27, var_138, var_44, var_168, var_132, var_7, var_158, var_165, var_123, var_127, var_1, var_8, var_29, var_156, var_111, var_140, var_37, var_14, var_80, var_56, var_92, var_55, var_118, var_22, var_142, var_157, var_198, var_95, var_79, var_16, var_26, var_49, var_72, var_96, var_175, var_13, var_189, var_69, var_159, var_73, var_125, var_41, var_85, var_121, var_107, var_148, var_18, var_74, var_116, var_128, var_150, var_11, var_119, var_129, var_53, var_64, var_131, var_99, var_110, var_144, var_117, var_5, var_172, var_180, var_82, var_184, var_185, var_36, var_98, var_169, var_143, var_162, var_84, var_145, var_51, var_6, var_166, var_45, var_87, var_164, var_137, var_197, var_171, var_193, var_199, var_66, var_58, var_122, var_178, var_70, var_167, var_182, var_47, var_187, var_35, var_108, var_91, var_20, var_86, var_34, var_139, var_160, var_183, var_23, var_154, var_194, var_0, var_48, var_60, var_77, var_83, var_103, var_33, var_100, var_93, var_50, var_68, var_75, var_54, var_101, var_24, var_176, var_155, var_130, var_88, var_25, var_192, var_39, var_97, var_163, var_62, var_17, var_186, var_59, var_76, var_191, var_106, var_57, var_170, var_42, var_152, var_46, var_61, var_81, var_89, var_105, var_135, var_28, var_52, var_63, var_174, var_4, var_104, var_15, var_146, var_173, var_115, var_65, var_40, var_161, var_179, var_190, var_32, var_124, var_38, var_3, var_12, var_153, var_109, var_71, var_120, var_94, var_31, var_181, var_196",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-cfc41b14483c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ID_code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, data, ntree_limit)\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"best_ntree_limit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[1;32m--> 681\u001b[1;33m                                                  ntree_limit=ntree_limit)\n\u001b[0m\u001b[0;32m    682\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"multi:softprob\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1478\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23'] ['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9', 'var_10', 'var_11', 'var_12', 'var_13', 'var_14', 'var_15', 'var_16', 'var_17', 'var_18', 'var_19', 'var_20', 'var_21', 'var_22', 'var_23', 'var_24', 'var_25', 'var_26', 'var_27', 'var_28', 'var_29', 'var_30', 'var_31', 'var_32', 'var_33', 'var_34', 'var_35', 'var_36', 'var_37', 'var_38', 'var_39', 'var_40', 'var_41', 'var_42', 'var_43', 'var_44', 'var_45', 'var_46', 'var_47', 'var_48', 'var_49', 'var_50', 'var_51', 'var_52', 'var_53', 'var_54', 'var_55', 'var_56', 'var_57', 'var_58', 'var_59', 'var_60', 'var_61', 'var_62', 'var_63', 'var_64', 'var_65', 'var_66', 'var_67', 'var_68', 'var_69', 'var_70', 'var_71', 'var_72', 'var_73', 'var_74', 'var_75', 'var_76', 'var_77', 'var_78', 'var_79', 'var_80', 'var_81', 'var_82', 'var_83', 'var_84', 'var_85', 'var_86', 'var_87', 'var_88', 'var_89', 'var_90', 'var_91', 'var_92', 'var_93', 'var_94', 'var_95', 'var_96', 'var_97', 'var_98', 'var_99', 'var_100', 'var_101', 'var_102', 'var_103', 'var_104', 'var_105', 'var_106', 'var_107', 'var_108', 'var_109', 'var_110', 'var_111', 'var_112', 'var_113', 'var_114', 'var_115', 'var_116', 'var_117', 'var_118', 'var_119', 'var_120', 'var_121', 'var_122', 'var_123', 'var_124', 'var_125', 'var_126', 'var_127', 'var_128', 'var_129', 'var_130', 'var_131', 'var_132', 'var_133', 'var_134', 'var_135', 'var_136', 'var_137', 'var_138', 'var_139', 'var_140', 'var_141', 'var_142', 'var_143', 'var_144', 'var_145', 'var_146', 'var_147', 'var_148', 'var_149', 'var_150', 'var_151', 'var_152', 'var_153', 'var_154', 'var_155', 'var_156', 'var_157', 'var_158', 'var_159', 'var_160', 'var_161', 'var_162', 'var_163', 'var_164', 'var_165', 'var_166', 'var_167', 'var_168', 'var_169', 'var_170', 'var_171', 'var_172', 'var_173', 'var_174', 'var_175', 'var_176', 'var_177', 'var_178', 'var_179', 'var_180', 'var_181', 'var_182', 'var_183', 'var_184', 'var_185', 'var_186', 'var_187', 'var_188', 'var_189', 'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195', 'var_196', 'var_197', 'var_198', 'var_199']\nexpected f16, f11, f5, f10, f15, f14, f23, f18, f2, f1, f4, f12, f21, f22, f20, f9, f8, f0, f3, f19, f13, f17, f7, f6 in input data\ntraining data did not have the following fields: var_78, var_114, var_9, var_19, var_147, var_177, var_134, var_90, var_21, var_30, var_141, var_67, var_102, var_151, var_133, var_136, var_188, var_195, var_113, var_149, var_43, var_2, var_126, var_112, var_10, var_27, var_138, var_44, var_168, var_132, var_7, var_158, var_165, var_123, var_127, var_1, var_8, var_29, var_156, var_111, var_140, var_37, var_14, var_80, var_56, var_92, var_55, var_118, var_22, var_142, var_157, var_198, var_95, var_79, var_16, var_26, var_49, var_72, var_96, var_175, var_13, var_189, var_69, var_159, var_73, var_125, var_41, var_85, var_121, var_107, var_148, var_18, var_74, var_116, var_128, var_150, var_11, var_119, var_129, var_53, var_64, var_131, var_99, var_110, var_144, var_117, var_5, var_172, var_180, var_82, var_184, var_185, var_36, var_98, var_169, var_143, var_162, var_84, var_145, var_51, var_6, var_166, var_45, var_87, var_164, var_137, var_197, var_171, var_193, var_199, var_66, var_58, var_122, var_178, var_70, var_167, var_182, var_47, var_187, var_35, var_108, var_91, var_20, var_86, var_34, var_139, var_160, var_183, var_23, var_154, var_194, var_0, var_48, var_60, var_77, var_83, var_103, var_33, var_100, var_93, var_50, var_68, var_75, var_54, var_101, var_24, var_176, var_155, var_130, var_88, var_25, var_192, var_39, var_97, var_163, var_62, var_17, var_186, var_59, var_76, var_191, var_106, var_57, var_170, var_42, var_152, var_46, var_61, var_81, var_89, var_105, var_135, var_28, var_52, var_63, var_174, var_4, var_104, var_15, var_146, var_173, var_115, var_65, var_40, var_161, var_179, var_190, var_32, var_124, var_38, var_3, var_12, var_153, var_109, var_71, var_120, var_94, var_31, var_181, var_196"
     ]
    }
   ],
   "source": [
    "test = test.drop([\"ID_code\"], axis=1)\n",
    "final = model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
